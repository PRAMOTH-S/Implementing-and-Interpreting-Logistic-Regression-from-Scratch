{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Implementing and Interpreting Logistic Regression from Scratch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, log_loss\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "z8SpsoPr222V"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=5,\n",
        "    n_informative=5,\n",
        "    n_redundant=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Zz5aHlV-3Aa2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, lr=0.01, n_iters=10000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def loss(self, y, y_hat):\n",
        "        return -np.mean(y*np.log(y_hat + 1e-9) + (1-y)*np.log(1-y_hat + 1e-9))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_output = np.dot(X, self.weights) + self.bias\n",
        "            y_hat = self.sigmoid(linear_output)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_hat - y))\n",
        "            db = (1/n_samples) * np.sum(y_hat - y)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            self.losses.append(self.loss(y, y_hat))\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_hat = self.sigmoid(linear_output)\n",
        "        return (y_hat >= 0.5).astype(int)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "      linear_output = np.dot(X, self.weights) + self.bias\n",
        "      return self.sigmoid(linear_output)\n"
      ],
      "metadata": {
        "id": "B7O0B_mN3Kfl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model = LogisticRegressionScratch()\n",
        "custom_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob_custom = custom_model.predict_proba(X_test)\n",
        "custom_loss = log_loss(y_test, y_prob_custom)\n",
        "y_pred_custom = custom_model.predict(X_test)"
      ],
      "metadata": {
        "id": "G7ek1pWz3OCO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_accuracy = accuracy_score(y_test, y_pred_custom)\n",
        "custom_precision = precision_score(y_test, y_pred_custom)\n",
        "custom_recall = recall_score(y_test, y_pred_custom)\n",
        "custom_loss = log_loss(y_test, y_pred_custom)\n",
        "\n",
        "custom_accuracy, custom_precision, custom_recall, custom_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZiSKzO_3PV_",
        "outputId": "d609a5f4-1014-4d3e-d420-08e8eae35886"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.825, 0.8118811881188119, 0.8367346938775511, 6.307639343095502)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_model = LogisticRegression()\n",
        "sk_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sk = sk_model.predict(X_test)"
      ],
      "metadata": {
        "id": "0bZk1RhA3Soy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = custom_model.weights\n",
        "\n",
        "sorted_indices = np.argsort(coefficients)\n",
        "top_negative = sorted_indices[:2]\n",
        "top_positive = sorted_indices[-2:]\n",
        "\n",
        "coefficients, top_positive, top_negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh6KsbDz3V4w",
        "outputId": "b7bf425f-d742-4d25-e40e-8dcfdf58202b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.76297217,  0.82739567,  0.72748989, -0.33026034, -0.74970028]),\n",
              " array([2, 1]),\n",
              " array([0, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}